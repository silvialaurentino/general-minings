# -*- coding: utf-8 -*-
"""Sumarização via URL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E5ErQ4ybgj0y-OOcNGfEoMRgqAUVE6_8
"""

import re
import nltk
import string
import heapq
from IPython.core.display import HTML
nltk.download('punkt')
nltk.download('stopwords')

!pip install goose3

from goose3 import Goose

g = Goose()
url = 'https://iaexpert.academy/2020/11/09/ia-preve-resultado-das-eleicoes-americanas/'
artigo = g.extract(url)

artigo.infos

original = artigo.cleaned_text
original

stopwords = nltk.corpus.stopwords.words('portuguese')

def preprocess(input):
  lowered = input.lower()
  tokens = []
  for token in nltk.word_tokenize(lowered):
    tokens.append(token)

  tokens = [palavra for palavra in tokens if palavra not in stopwords and palavra not in string.punctuation]
  
  formatado = ' '.join([str(elemento) for elemento in tokens if not elemento.isdigit()])

  return formatado

def sumariza(texto, num_sent):
  original = texto
  formatado = preprocess(original)

  freq = nltk.FreqDist(nltk.word_tokenize(formatado))
  maxfreq = max(freq.values())

  for palavra in freq.keys():
    freq[palavra] = (freq[palavra] / maxfreq)
  sentencas = nltk.sent_tokenize(original)

  nota = {}
  for sentenca in sentencas:
    for palavra in nltk.word_tokenize(sentenca):
      if palavra in freq.keys():
        if sentenca not in nota.keys():
          nota[sentenca] = freq[palavra]
        else:
          nota[sentenca] += freq[palavra]

  melhores = heapq.nlargest(num_sent, nota, key=nota.get)

  return sentencas, melhores, freq, nota

sentencas, melhores, freq, nota = sumariza(original,5)

def resumo(titulo, sentencas, melhores):
   from IPython.core.display import HTML
   texto = ''

   display(HTML(f'<h1>Resumo do texto - {titulo}</h1>'))
   for i in sentencas:
     if i in melhores:
       texto += str(i).replace(i, f"<mark>{i}</mark>")
     else:
       texto += i
   display(HTML(f""" {texto} """))

resumo('Eleições', sentencas, melhores)

artigos = ['https://iaexpert.academy/2020/11/06/ia-detecta-deep-fakes-produzidos-com-tecnicas-recentes/',
           'https://iaexpert.academy/2016/07/19/historico-da-ia-teste-de-turing/',
           'https://iaexpert.academy/2017/04/05/3-linguagens-para-inteligencia-artificial/']

for url in artigos:
  g = Goose()
  artigo = g.extract(url)
  sentencas, melhores, _, _, = sumariza(artigo.cleaned_text, 7)
  resumo(artigo.title, sentencas, melhores)

import spacy
!python -m spacy download pt

pln = spacy.load('pt')
pln

doc = pln()

def lema(texto):
  texto = texto.lower()
  texto = re.sub(r' +', ' ', texto)

  doc = pln(texto)
  tokens = []
  for token in doc:
    tokens.append(token.lemma_)

  tokens = [palavra for palavra in tokens if palavra not in stopwords and palavra not in string.punctuation]
  formatado = ' '.join([str(elemento) for elemento in tokens if not elemento.isdigit()])

  return formatado

lema(original)

def sumlem(texto, qt):
  original = texto
  formatado = lema(original)

  freq = nltk.FreqDist(nltk.word_tokenize(formatado))
  maxfreq = max(freq.values())

  for palavra in freq.keys():
    freq[palavra] = (freq[palavra] / maxfreq)
  sentencas = nltk.sent_tokenize(original)

  notas = {}
  for sentenca in sentencas:
    for palavra in nltk.word_tokenize(sentenca):
      if palavra in freq.keys():
        if sentenca not in notas.keys():
          notas[sentenca] = freq[palavra]
        else:
          notas[sentenca] += freq[palavra]

  import heapq
  melhores = heapq.nlargest(qt, notas, key=notas.get)

  return sentencas, melhores, freq, notas

for url in artigos:
  g = Goose()
  artigo = g.extract(url)
  sentencas, melhores, _, _ = sumlem(artigo.cleaned_text, 5)
  resumo(artigo.title, sentencas, melhores)