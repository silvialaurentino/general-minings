# -*- coding: utf-8 -*-
"""Algoritmo Luhn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rmmhbydo73BntWQzpGfxWSeUHU3H19r1
"""

import re
import nltk
import numpy
import string
import heapq

nltk.download('punkt')
nltk.download('stopwords')

original = """A inteligência artificial é a inteligência similar à humana máquinas. 
                    Definem como o estudo de agente artificial com inteligência. 
                    Ciência e engenharia de produzir máquinas com inteligência. 
                    Resolver problemas e possuir inteligência. 
                    Relacionada ao comportamento inteligente. 
                    Construção de máquinas para raciocinar. 
                    Aprender com os erros e acertos. 
                    Inteligência artificial é raciocinar nas situações do cotidiano."""
original = re.sub(r'\s+', ' ', original)
original

stopwords = nltk.corpus.stopwords.words('portuguese')
stopwords.append('ser')

def prepross(texto):
  lowered = texto.lower()
  tokens = []
  for token in nltk.word_tokenize(lowered):
    tokens.append(token)

  tokens = [palavra for palavra in tokens if palavra not in stopwords and palavra not in string.punctuation]
  formatado = ' '.join([str(elemento) for elemento in tokens if not elemento.isdigit()])

  return formatado

def notasent(sentencas, importantes, dist):
  notas = []
  id_sent = 0

  for sentenca in [nltk.word_tokenize(sentenca.lower()) for sentenca in sentencas]:
    
    id_p = []
    for palavra in importantes:
      if palavra in sentenca:
        id_p.append(sentenca.index(palavra))

    id_p.sort()

    if len(id_p) != 0:
      grupos = []
      grupo = [id_p[0]]
      i = 1
      while i < len(id_p):
        if (id_p[i] - id_p[i - 1]) < dist:
          grupo.append(id_p[i])
        else:
          grupos.append(grupo)
          grupo = [id_p[i]]
        i +=1
      grupos.append(grupo)

      maxnota = 0
      for g in grupos:
        impgrupo = len(g)
        total = g[-1] - g[0] + 1
        nota = 1.0 * impgrupo**2 / total

        if nota > maxnota:
          maxnota = nota

      notas.append((maxnota, id_sent))
      id_sent += 1
  return notas

def sum(texto, n, dist, qt):
  originais = [sentenca for sentenca in nltk.sent_tokenize(texto)]
  formatadas = [prepross(original) for original in originais]
  palavras = [palavra for sentenca in formatadas for palavra in nltk.word_tokenize(sentenca)]
  freq = nltk.FreqDist(palavras)
  top = [palavra[0] for palavra in freq.most_common(n)]

  notas = notasent(formatadas, top, dist)
  melhores = heapq.nlargest(qt, notas)
  
  melhores = [originais[i] for (nota, i) in melhores]
  return originais, melhores, notas

originais, melhores, notas = sum(original, 5, 3, 3)

originais

melhores

notas

def resumo(titulo, sentencas, melhores):
  from IPython.core.display import HTML
  texto = ''

  display(HTML(f'<h1>Resumo - {titulo}</h1>'))
  for i in sentencas:
    if i in melhores:
      texto += str(i).replace(i, f"<mark>{i}</mark>")
    else:
      texto += i
  display(HTML(f" {texto} "))

resumo('Teste', originais, melhores)

!pip install goose3

from goose3 import Goose

g = Goose()
url = 'https://iaexpert.academy/2020/11/09/ia-preve-resultado-das-eleicoes-americanas/'
artigo = g.extract(url)

artigo.cleaned_text

originais, melhores, notas = sum(artigo.cleaned_text, 100, 5, 5)

notas

resumo(artigo.title, originais, melhores)

!pip install feedparser

import feedparser
from bs4 import BeautifulSoup
import os
import json

url = 'http://iaexpert.academy/feed'
feed = feedparser.parse(url)

for e in feed.entries:
  print(e.title)
  print(e.links[0].href)
  print(e.content[0].value)

def clean(texto):
  if texto == '':
    return ''
  return BeautifulSoup(texto, 'html5lib').get_text()

clean(e.content[0].value)

artigos = []
for e in feed.entries:
  artigos.append({'titulo': e.title, 'conteudo': clean(e.content[0].value)})

artigos

salvar = os.path.join('feed_iaexpert.json')
arquivo = open(salvar, 'w+')
arquivo.write(json.dumps(artigos, indent=1))
arquivo.close()

artigos = json.loads(open('/content/feed_iaexpert.json').read())
artigos

feed = ''
for artigo in artigos:
  feed += artigo['conteudo']

feed

feed_formatado = prepross(feed)
feed_formatado

len(feed), len(feed_formatado)

from wordcloud import WordCloud
import matplotlib.pyplot as plt
plt.figure(figsize=(20,20))
plt.axis('off')
plt.imshow(WordCloud().generate(feed_formatado))

import spacy
!python -m spacy download pt

pln = spacy.load('pt')
pln

documento = pln(feed_formatado)

from spacy import displacy
displacy.render(documento, style = 'ent', jupyter = True)

for entidade in documento.ents:
  if entidade.label_ == 'ORG':
    print(entidade.text, entidade.label_)

for artigo in artigos:
  originais, melhores, _ = sum(artigo['conteudo'], 150, 10, 5)
  resumo(artigo['titulo'], originais, melhores)

def salva_resumo(titulo, sentencas, melhores):
  HTML_TEMPLATE = """<html>
    <head>
      <title>{0}</title>
      <meta http-equiv="Content-Type" content="text/html; charset=UTL-8" />
    </head>
    <body>{1}</body>
  </html>"""

  texto = ''
  for i in sentencas:
    if i in melhores:
      texto += str(i).replace(i, f'<mark>{i}</mark>')
    else:
      texto += i
  arquivo = open(os.path.join(titulo + '.html'), 'wb')
  html = HTML_TEMPLATE.format(titulo + ' - resumo', texto)
  arquivo.write(html.encode('utf-8'))
  arquivo.close()

for artigo in artigos:
  originais, melhores, _ = sum(artigo['conteudo'], 150, 10, 5)
  salva_resumo(artigo['titulo'], originais, melhores)

def preprosslem(texto):
  lowered = texto.lower()
  texto = re.sub(r' +', ' ', texto)

  doc = pln(texto)
  tokens = []
  for token in doc:
    tokens.append(token.lemma_)

  tokens = [palavra for palavra in tokens if palavra not in stopwords and palavra not in string.punctuation]
  formatado = ' '.join([str(elemento) for elemento in tokens if not elemento.isdigit()])

  return formatado

def sumlem(texto, n, dist, qt):
  originais = [sentenca for sentenca in nltk.sent_tokenize(texto)]
  formatadas = [preprosslem(original) for original in originais]
  palavras = [palavra for sentenca in formatadas for palavra in nltk.word_tokenize(sentenca)]
  freq = nltk.FreqDist(palavras)
  top = [palavra[0] for palavra in freq.most_common(n)]

  notas = notasent(formatadas, top, dist)
  melhores = heapq.nlargest(qt, notas)
  
  melhores = [originais[i] for (nota, i) in melhores]
  return originais, melhores, notas

artigos[0]['conteudo']

originais, melhores, _ = sumlem(artigos[0]['conteudo'], 300, 5, 5)
resumo(artigos[0]['titulo'], originais, melhores)